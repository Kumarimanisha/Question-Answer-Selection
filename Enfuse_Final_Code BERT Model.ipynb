{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v4tklD5SIZUm",
    "outputId": "cc3ae8c6-1d87-4526-aa71-d338053cbdd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
      "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L-vAoGy3zsjE"
   },
   "outputs": [],
   "source": [
    "'''Importing required libraries and downloading necessary modules'''\n",
    "import torch\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "import sys\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "'''creating instance for  pre-trained BERT model fine-tuned on the Stanford Question Answering Dataset (SQuAD) \n",
    "for question answering process  '''\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "'''creating instance for BertTokenizer which is used to convert input text into tokens \n",
    "that the BERT model can process.'''\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3NCJjee3z-sY"
   },
   "outputs": [],
   "source": [
    "'''function to read data from wikipedia URL one-by-one and writing in one single dataframe'''\n",
    "def read_wikipedia(file_path):\n",
    "    \"\"\"Scraping text data from the webpages given in url_list file\"\"\"\n",
    "    df_final=pd.DataFrame()  \n",
    "    doc_id=1 \n",
    "    # get URL\n",
    "    with open(file_path, \"r\") as url_list: \n",
    "        for url in url_list:\n",
    "            url=url.strip()\n",
    "            page = requests.get(url)    \n",
    "            # scrape webpage\n",
    "            soup = BeautifulSoup(page.content, 'html.parser') \n",
    "            '''Reading only paragraphs from the wikipedia pages'''\n",
    "            p_tags=soup.find_all(\"p\")\n",
    "            li_tags=soup.find_all(\"li\")\n",
    "            p_lines= [p.get_text() for p in p_tags]\n",
    "            li_lines =[l.get_text() for l in li_tags]\n",
    "            lines=[]\n",
    "            app1 = [line for line in p_lines if (line!='\\n' and line!='')]\n",
    "            app2= [line for line in li_lines if (line!='\\n' and line!='')]\n",
    "            lines=app1+app2\n",
    "            df=pd.DataFrame(lines,columns=['paragraph'])    \n",
    "            df['paragraph']=df['paragraph'].str.replace(r\"\\[\\d+\\]*\",'',regex=True)\n",
    "            '''doc_id created to identify different wikipedia pages read'''\n",
    "            df['doc_id'] = doc_id\n",
    "            doc_name=url.split(\"/\")[-1]\n",
    "            '''document_name created to store the name of wikipedia page'''\n",
    "            df['document_name']=doc_name\n",
    "            doc_id+=1       \n",
    "            df['paragraph_breakdown']=df['paragraph'].apply(lambda x: len(x.split()))\n",
    "            df=df[df['paragraph_breakdown'] > 80]\n",
    "            '''Paragraph_id created to uniquely identify paragraphs from the page'''\n",
    "            df['paragraph_id'] = np.arange(len(df))\n",
    "            df_final = pd.concat([df_final, df])\n",
    "            \n",
    "    '''creating final dataframe to write the data fetched from wikedia pages'''\n",
    "    df_final = df_final[['doc_id','document_name','paragraph_id','paragraph']]    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JFl8cYz6z_XW"
   },
   "outputs": [],
   "source": [
    "'''function to remove stop words from a sentence'''\n",
    "def remove_stop_words(sentence):\n",
    "    # Tokenize the sentence into words\n",
    "    words = word_tokenize(sentence)\n",
    "    \n",
    "    # Remove stop words from the sentence\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    \n",
    "    # Join the filtered words back into a sentence\n",
    "    filtered_sentence = ' '.join(filtered_words)\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d1_Td3biH6Ht"
   },
   "outputs": [],
   "source": [
    "'''function to lemmatize a sentence'''\n",
    "def lemmatize_sentence(sentence):\n",
    "    # Initialize the lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # Tokenize the sentence into words\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    \n",
    "    # Lemmatize each word using its part of speech tag\n",
    "    lemmas = []\n",
    "    for word, tag in nltk.pos_tag(words):\n",
    "        pos = get_wordnet_pos(tag)\n",
    "        lemma = lemmatizer.lemmatize(word, pos=pos)\n",
    "        lemmas.append(lemma)\n",
    "        \n",
    "    # Join the lemmatized words back into a sentence\n",
    "    lemmatized_sentence = ' '.join(lemmas)\n",
    "    return lemmatized_sentence\n",
    "\n",
    "'''function to map the part of speech tags returned by nltk.pos_tag to the WordNet part of speech categories'''\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # default to noun if the tag is not recognized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Fz0UfSm0WlI"
   },
   "outputs": [],
   "source": [
    "'''function to pre-process the dataframe'''\n",
    "def pre_processing(df,column):\n",
    "    df['pre_processed_text']=df[column].str.replace(r\"\\[\\d+\\]*\",'',regex=True)      \n",
    "    df['pre_processed_text'] = df['pre_processed_text'].apply(remove_stop_words)\n",
    "    df['pre_processed_text']=df['pre_processed_text'].str.replace(r'[^\\w\\s]+','',regex=True).str.lower() \n",
    "    df['pre_processed_text'] = df['pre_processed_text'].apply(lambda x: lemmatize_sentence(x))\n",
    "    #df=df[['doc_id','document_name','paragraph_id','description','pre_processed_desc']]   \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZPvQSdUS0YVO"
   },
   "outputs": [],
   "source": [
    "def segment_documents(df, max_doc_length=450):\n",
    "    \"\"\" Create a new DataFrame to store segmented documents; this will help in pulling the specific keywords which made the \n",
    "    specific line to be suitable for a query\"\"\"    \n",
    "    segmented_docs = pd.DataFrame(columns=['doc_id', 'document_name', 'paragraph_id', 'paragraph', 'document'])\n",
    "\n",
    "    # Iterate over each row in the input DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        doc_id = row['doc_id']\n",
    "        document_name = row['document_name']\n",
    "        paragraph_id = row['paragraph_id']\n",
    "        doc = row['paragraph']\n",
    "        lemmatize = row['pre_processed_text']\n",
    "        # Split document by spaces to obtain a word count that roughly approximates the token count\n",
    "        split_to_words = lemmatize.split(\" \")\n",
    "\n",
    "        # If the document is longer than our maximum length, split it up into smaller segments and add them to the new DataFrame\n",
    "        if len(split_to_words) > max_doc_length:\n",
    "            for doc_segment in range(0, len(split_to_words), max_doc_length):\n",
    "                segmented_docs = pd.concat([segmented_docs, pd.DataFrame({'doc_id': [doc_id], 'document_name': [document_name], 'paragraph_id': [paragraph_id], 'paragraph': [doc] ,'document': [\" \".join(split_to_words[doc_segment:doc_segment + max_doc_length])]})])\n",
    "\n",
    "        # If the document is shorter than our maximum length, add it to the new DataFrame\n",
    "        else:\n",
    "            segmented_docs = pd.concat([segmented_docs, pd.DataFrame({'doc_id': [doc_id], 'document_name': [document_name], 'paragraph_id': [paragraph_id], 'paragraph': [doc] ,'document': [lemmatize]})])\n",
    "\n",
    "    return segmented_docs.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AP_KtjZN0anp"
   },
   "outputs": [],
   "source": [
    "'''function to pull top k sections based on the keywords in question'''\n",
    "def get_top_k_articles(query, docs_df, ans_column, k=5):\n",
    "    # Remove stop words from the query and lemmatize it\n",
    "    filtered_query = remove_stop_words(query)\n",
    "    lemmatized_query = lemmatize_sentence(filtered_query)\n",
    "\n",
    "    # Initialize a vectorizer that uses the preprocessed document text\n",
    "    vectorizer = TfidfVectorizer(analyzer=\"word\")\n",
    "\n",
    "    # Create a corpus of query and documents and convert to TFIDF vectors\n",
    "    query_and_docs = [lemmatized_query] + list(docs_df[ans_column])    \n",
    "    matrix = vectorizer.fit_transform(query_and_docs)\n",
    "\n",
    "    # Holds our cosine similarity scores\n",
    "    scores = []\n",
    "\n",
    "    # The first vector is our query text, so compute the similarity of our query against all document vectors\n",
    "    for i in range(1, len(query_and_docs)):\n",
    "        score = cosine_similarity(matrix[0], matrix[i])[0][0]\n",
    "        scores.append(score)\n",
    "\n",
    "    # Check if all scores are 0 and return an empty dataframe if so\n",
    "    if all(score ==0 for score in scores):\n",
    "        return pd.DataFrame(columns=['doc_id', 'document_name', 'paragraph_id', 'paragraph', 'document'])\n",
    "\n",
    "    # Sort list of scores and return the top k highest scoring documents\n",
    "    sorted_list = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)\n",
    "    top_doc_indices = [x[0] for x in sorted_list[:k]]\n",
    "    top_docs = docs_df.iloc[top_doc_indices]\n",
    "  \n",
    "    return top_docs.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kpIR32T20ilj"
   },
   "outputs": [],
   "source": [
    "'''function to find releavnt answer using BERT model'''\n",
    "def bert_answer(question, candidate, query_id, display= False):\n",
    "    query = question.replace(\"?\",'')\n",
    "    filtered_query = remove_stop_words(query)\n",
    "    lemmatized_query = lemmatize_sentence(filtered_query)\n",
    "    \n",
    "    # Initialize variables for best answer and document information\n",
    "    max_score = float('-inf')\n",
    "    best_answer = \"\"\n",
    "    doc_id = \"\"\n",
    "    document_name = \"\"\n",
    "    paragraph_id = \"\"\n",
    "    paragraph = \"\"\n",
    "    \n",
    "    # Loop through the candidate documents and find the one with the highest score\n",
    "    for _, row in candidate.iterrows():\n",
    "        # Tokenize the question and document\n",
    "        inputs = tokenizer(lemmatized_query, row[\"document\"], add_special_tokens=True, return_tensors=\"pt\")\n",
    "        input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "        token_type_ids = [0] * len(input_ids)\n",
    "        tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "        start_scores, end_scores = model(**inputs).start_logits, model(**inputs).end_logits\n",
    "        start_index = torch.argmax(start_scores)\n",
    "        end_index = torch.argmax(end_scores) + 1\n",
    "        answer_tokens = tokens[start_index:end_index]\n",
    "        answer_tokens = tokenizer.convert_tokens_to_string(answer_tokens)\n",
    "        score = start_scores[0][start_index] + end_scores[0][start_index]\n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "            best_answer = answer_tokens\n",
    "            doc_id = row[\"doc_id\"]\n",
    "            document_name = row[\"document_name\"]\n",
    "            paragraph_id = row[\"paragraph_id\"]\n",
    "            paragraph = row[\"paragraph\"]\n",
    "    if display:\n",
    "        promptDf = pd.DataFrame({'doc_id': [doc_id], 'document_name': [document_name], 'query_id': [query_id], 'query': [question], 'paragraph_id': [paragraph_id], 'answer': [paragraph], 'keyword' : [best_answer], 'bert_score': [max_score.detach().numpy()]}) \n",
    "        return promptDf\n",
    "    else:\n",
    "        storeDf = pd.DataFrame({'doc_id': [doc_id], 'document_name': [document_name], 'query_id': [query_id], 'query': [question], 'paragraph_id': [paragraph_id], 'answer': [paragraph], 'keyword' : [best_answer], 'bert_score': [max_score.detach().numpy()]}) \n",
    "        return storeDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k-O2dcnJ0jXR"
   },
   "outputs": [],
   "source": [
    "'''function to generate final output '''\n",
    "def result(query_df, segmented_docs):\n",
    "    resultDf = pd.DataFrame(columns=['doc_id', 'document_name', 'query_id', 'query', 'paragraph_id', 'answer', 'keyword', 'bert_score'])\n",
    "    # Loop through each query\n",
    "    for index, row in query_df.iterrows():\n",
    "        query = row['query']\n",
    "        query_id = row['query_id']\n",
    "    \n",
    "        print(\"Processing answer for query \" + str(int(query_id))+\": \"+ query)\n",
    "        # Segment our documents\n",
    "        # Retrieve the top k most relevant documents to the query\n",
    "        candidate_docs = get_top_k_articles(query, segmented_docs, 'document', 5)\n",
    "        if candidate_docs.empty:\n",
    "            new_row = pd.DataFrame({'doc_id': '', 'document_name': '',  'query_id' : [query_id], 'query': [query],'paragraph_id': '', 'answer': \"No answer.\", 'keyword' : '', 'bert_score': 0})\n",
    "            new_row.reset_index(drop=True, inplace=True)\n",
    "            resultDf = pd.concat([resultDf,new_row])\n",
    "            continue\n",
    "        # Return the likeliest answers from each of our top k most relevant documents in descending order\n",
    "        result_df = bert_answer(query, candidate_docs, query_id)\n",
    "        resultDf = pd.concat([resultDf, result_df]).reset_index(drop=True)\n",
    "    return resultDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dt2zScuN0mx2"
   },
   "outputs": [],
   "source": [
    "'''manin function created to call model on whole annotaed dataset'''\n",
    "def main():\n",
    "    soup_list=[]\n",
    "    '''reading input file having list of wikipedia pages URL to fetch data from'''\n",
    "    file_path='url_lists.txt'\n",
    "    df=read_wikipedia(file_path)\n",
    "\n",
    "    df=pre_processing(df,'paragraph')\n",
    "    segmented_docs = segment_documents(df, 450)\n",
    "    \n",
    "    query_df = pd.read_csv('Annotated_queries.csv', encoding='iso-8859-1')\n",
    "    print(\"Approach 1\")\n",
    "    resultDf = result(query_df, segmented_docs)\n",
    "    resultDf.to_csv('enfuse_answer.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EdGV8mh80piw"
   },
   "outputs": [],
   "source": [
    "'''function prompt can be called to enable user prompt mode'''\n",
    "def prompt():\n",
    "    soup_list=[]\n",
    "    '''reading input file having list of wikipedia pages URL to fetch data from'''\n",
    "    file_path='url_lists.txt'\n",
    "    df=read_wikipedia(file_path)\n",
    "\n",
    "    df=pre_processing(df,'paragraph')\n",
    "    segmented_docs = segment_documents(df, 450)\n",
    "    i=0\n",
    "    print(\"Welcome to the QA system!\")\n",
    "    while True:\n",
    "        # Prompt for user input\n",
    "        question = input(\"What's your question? (Press q to quit) \")\n",
    "        if question.lower() == 'q':\n",
    "            break\n",
    "        i = i+1\n",
    "        print(\"Processing answer for query: \" +question)\n",
    "        print(\"-------------------------------------------------\")            \n",
    "        # Get the answer and display it\n",
    "        candidate_docs = get_top_k_articles(question, segmented_docs,  'document',  5)\n",
    "        if candidate_docs.empty:\n",
    "            print(question+\" \"+\"No answer\")\n",
    "            continue\n",
    "        # Return the likeliest answers from each of our top k most relevant docments in descending order\n",
    "        final_answer = bert_answer(question, candidate_docs, i, display=True)\n",
    "        print(\"Most relevant answer for the question\")\n",
    "        print(\"-----------------------DOCUMENT NAME--------------------------\")\n",
    "        print(final_answer['document_name'].to_string(index=False))\n",
    "        print(\"-----------------------MATCHED KEYWORDS--------------------------\")\n",
    "        print(final_answer['keyword'].to_string(index=False))\n",
    "        print(\"-----------------------PARAGRAPH--------------------------\")\n",
    "        print( final_answer['answer'].to_string(index=False))\n",
    "    print(\"Thanks for using the QA system!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Ibc8acLWGnK"
   },
   "outputs": [],
   "source": [
    "'''Main function can be called to create output dataset for all the queries in one go and can be used for\n",
    "model evaluation phase'''\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''calling the user prompt function'''\n",
    "prompt()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
